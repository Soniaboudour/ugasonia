# Fonction pour lire un fichier CoNLL-U et calculer les statistiques de base et générer un fichier JSON
import json
from collections import Counter

def read_conllu(file_path, ngram_min_length=3, ngram_max_length=6):
    try:
        # Ouvrir le fichier
        with open(file_path, mode="r", encoding="utf-8") as file:
            nbToks = 0
            nbSents = 0
            nbForms = 0
            nbPuncts = 0
            forms_set = set()
            noun_counter = Counter()
            verb_counter = Counter()
            adj_counter = Counter()
            adv_counter = Counter()
            lem_counter = Counter()
            sentence_forms = []
            total_form_length = 0
            total_sent_length = 0
            ngrams = {n: Counter() for n in range(ngram_min_length, ngram_max_length + 1)}

            # Lire ligne par ligne
            for line in file:
                line = line.strip()

                # Si la ligne commence par un #, c'est un commentaire (phrase)
                if line.startswith("#"):
                    if "# text =" in line:
                        nbSents += 1
                        total_sent_length += len(sentence_forms)
                        if sentence_forms:
                            for n in range(ngram_min_length, ngram_max_length + 1):
                                for i in range(len(sentence_forms) - n + 1):
                                    ngram = tuple(sentence_forms[i:i + n])
                                    ngrams[n][ngram] += 1
                            sentence_forms = []
                elif line:  # Ligne non vide et non commentaire
                    if not line.startswith("#"):
                        parts = line.split("\t")
                        if len(parts) > 3:
                            form = parts[1]
                            lemma = parts[2]
                            upos = parts[3]

                            nbToks += 1
                            nbForms += 1
                            total_form_length += len(form)
                            forms_set.add(form)
                            lem_counter[lemma] += 1
                            sentence_forms.append(form)

                            if upos == "PUNCT":
                                nbPuncts += 1
                            elif upos == "NOUN":
                                noun_counter[form] += 1
                            elif upos == "VERB":
                                verb_counter[form] += 1
                            elif upos == "ADJ":
                                adj_counter[form] += 1
                            elif upos == "ADV":
                                adv_counter[form] += 1

            # Gérer les ngrammes pour la dernière phrase
            if sentence_forms:
                total_sent_length += len(sentence_forms)
                for n in range(ngram_min_length, ngram_max_length + 1):
                    for i in range(len(sentence_forms) - n + 1):
                        ngram = tuple(sentence_forms[i:i + n])
                        ngrams[n][ngram] += 1

            nbTypes = len(forms_set)
            averageSentLength = total_sent_length / nbSents if nbSents > 0 else 0
            averageFormLength = total_form_length / nbForms if nbForms > 0 else 0

            noun2freq = noun_counter.most_common()
            verb2freq = verb_counter.most_common()
            adj2freq = adj_counter.most_common()
            adv2freq = adv_counter.most_common()
            lem2freq = lem_counter.most_common()

            ngram_results = {n: ngrams[n].most_common() for n in range(ngram_min_length, ngram_max_length + 1)}

            return {
                "nbToks": nbToks,
                "nbSents": nbSents,
                "nbForms": nbForms,
                "nbPuncts": nbPuncts,
                "nbTypes": nbTypes,
                "averageSentLength": averageSentLength,
                "averageFormLength": averageFormLength,
                "noun2freq": noun2freq,
                "verb2freq": verb2freq,
                "adj2freq": adj2freq,
                "adv2freq": adv2freq,
                "lem2freq": lem2freq,
                "ngrams": ngram_results
            }
    except Exception as e:
        print(f"Erreur lors de la lecture du fichier : {e}")
        return None

def write_json(output_path, data):
    try:
        with open(output_path, mode="w", encoding="utf-8") as file:
            json.dump(data, file, indent=4, ensure_ascii=False)
        print(f"Fichier JSON généré avec succès : {output_path}")
    except Exception as e:
        print(f"Erreur lors de l'écriture du fichier JSON : {e}")

# Exemple d'utilisation
file_path = "/mnt/data/fr_sequoia-ud-dev.conllu"
output_path = "/mnt/data/output.json"
result = read_conllu(file_path)
if result:
    write_json(output_path, result)
