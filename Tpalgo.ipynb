{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soniaboudour/ugasonia/blob/main/Tpalgo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Fonction pour analyser le fichier Conllu\n",
        "def parse_conllu(file_path):\n",
        "    sentences = []\n",
        "    current_sentence = {\"tokens\": [], \"text\": \"\"}\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            if line.startswith(\"# text =\"):\n",
        "                # Capturer le texte de la phrase\n",
        "                current_sentence[\"text\"] = line.split(\"=\", 1)[1].strip()\n",
        "            elif line.startswith(\"#\") or not line.strip():\n",
        "                # Ignorer les métadonnées ou lignes vides\n",
        "                continue\n",
        "            else:\n",
        "                # Ligne de token\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) == 10:  # Valider les lignes de tokens\n",
        "                    try:\n",
        "                        # Attempt to convert to integer, if fails, keep as string\n",
        "                        token_id = int(parts[0])\n",
        "                    except ValueError:\n",
        "                        token_id = parts[0]\n",
        "\n",
        "                    token_data = {\n",
        "                        \"id\": token_id,  # Use the potentially modified token_id\n",
        "                        \"form\": parts[1],\n",
        "                        \"lemma\": parts[2],\n",
        "                        \"upos\": parts[3],\n",
        "                        \"xpos\": parts[4],\n",
        "                        \"feats\": parts[5],\n",
        "                        \"head\": int(parts[6]) if parts[6].isdigit() else None,\n",
        "                        \"deprel\": parts[7],\n",
        "                        \"deps\": parts[8],\n",
        "                        \"misc\": parts[9],\n",
        "                    }\n",
        "                    current_sentence[\"tokens\"].append(token_data)\n",
        "\n",
        "            # Fin d'un bloc de phrase\n",
        "            if line.strip() == \"\":\n",
        "                if current_sentence[\"tokens\"]:\n",
        "                    sentences.append(current_sentence)\n",
        "                    current_sentence = {\"tokens\": [], \"text\": \"\"}\n",
        "\n",
        "    # Ajouter la dernière phrase si elle existe\n",
        "    if current_sentence[\"tokens\"]:\n",
        "        sentences.append(current_sentence)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Fonction pour écrire les résultats au format JSON\n",
        "def write_json(data, output_path):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Chemin vers le fichier Conllu et fichier de sortie\n",
        "input_file = \"/content/fr_sequoia-ud-dev.conllu\"  # Remplace par le chemin réel\n",
        "output_file = \"résultat.json\"  # Chemin pour le fichier JSON\n",
        "\n",
        "# Analyse et écriture\n",
        "parsed_data = parse_conllu(input_file)\n",
        "write_json(parsed_data, output_file)\n",
        "\n",
        "print(f\"Données transformées en JSON et écrites dans {output_file}.\") import json\n",
        "\n",
        "# Fonction pour analyser le fichier Conllu\n",
        "def parse_conllu(file_path):\n",
        "    sentences = []\n",
        "    current_sentence = {\"tokens\": [], \"text\": \"\"}\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            if line.startswith(\"# text =\"):\n",
        "                # Capturer le texte de la phrase\n",
        "                current_sentence[\"text\"] = line.split(\"=\", 1)[1].strip()\n",
        "            elif line.startswith(\"#\") or not line.strip():\n",
        "                # Ignorer les métadonnées ou lignes vides\n",
        "                continue\n",
        "            else:\n",
        "                # Ligne de token\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) == 10:  # Valider les lignes de tokens\n",
        "                    try:\n",
        "                        # Attempt to convert to integer, if fails, keep as string\n",
        "                        token_id = int(parts[0])\n",
        "                    except ValueError:\n",
        "                        token_id = parts[0]\n",
        "\n",
        "                    token_data = {\n",
        "                        \"id\": token_id,  # Use the potentially modified token_id\n",
        "                        \"form\": parts[1],\n",
        "                        \"lemma\": parts[2],\n",
        "                        \"upos\": parts[3],\n",
        "                        \"xpos\": parts[4],\n",
        "                        \"feats\": parts[5],\n",
        "                        \"head\": int(parts[6]) if parts[6].isdigit() else None,\n",
        "                        \"deprel\": parts[7],\n",
        "                        \"deps\": parts[8],\n",
        "                        \"misc\": parts[9],\n",
        "                    }\n",
        "                    current_sentence[\"tokens\"].append(token_data)\n",
        "\n",
        "            # Fin d'un bloc de phrase\n",
        "            if line.strip() == \"\":\n",
        "                if current_sentence[\"tokens\"]:\n",
        "                    sentences.append(current_sentence)\n",
        "                    current_sentence = {\"tokens\": [], \"text\": \"\"}\n",
        "\n",
        "    # Ajouter la dernière phrase si elle existe\n",
        "    if current_sentence[\"tokens\"]:\n",
        "        sentences.append(current_sentence)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Fonction pour écrire les résultats au format JSON\n",
        "def write_json(data, output_path):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Chemin vers le fichier Conllu et fichier de sortie\n",
        "input_file = \"/content/fr_sequoia-ud-dev.conllu\"  # Remplace par le chemin réel\n",
        "output_file = \"résultat.json\"  # Chemin pour le fichier JSON\n",
        "\n",
        "# Analyse et écriture\n",
        "parsed_data = parse_conllu(input_file)\n",
        "write_json(parsed_data, output_file)\n",
        "\n",
        "print(f\"Données transformées en JSON et écrites dans {output_file}.\")"
      ],
      "metadata": {
        "id": "MCZtMmhno-Tn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}